{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e4bcdd",
   "metadata": {},
   "source": [
    "## What is RAG ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27668802",
   "metadata": {},
   "source": [
    "RAG is a technique where a language model first looks up (retrieves) useful information from a database or documents, and then uses that information to give a better answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee7c36",
   "metadata": {},
   "source": [
    "## Why and when we prefer RAG over finetuning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632852af",
   "metadata": {},
   "source": [
    "We **prefer RAG over finetuning** when: We want the model to give **up-to-date or specific answers** from **our own data** without changing the model itself.\n",
    "\n",
    "---\n",
    "\n",
    "**Why prefer RAG?**\n",
    "\n",
    "* **Cheaper & faster** – No need to train the model again.\n",
    "* **Easier to update** – Just change the documents, not the model.\n",
    "* **Better for private or large data** – You keep data separate and safe.\n",
    "\n",
    "---\n",
    "\n",
    "**When to use RAG?**\n",
    "\n",
    "* When your data **changes often** (like news, product lists).\n",
    "* When you want the model to **answer from your documents**.\n",
    "* When **training a model is too costly or slow**.\n",
    "\n",
    "---\n",
    "\n",
    "Think of RAG like **giving the model a library to read** instead of teaching it everything from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac356886",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0436fd6",
   "metadata": {},
   "source": [
    "* **`langchain`** – Core framework to build LLM-powered applications.\n",
    "* **`langchain-community`** – Extra integrations like tools, APIs, and vector stores.\n",
    "* **`langchain-pinecone`** – Connects LangChain with Pinecone for vector storage and retrieval.\n",
    "* **`langchain_groq`** – Enables LangChain to use Groq's ultra-fast language models.\n",
    "* **`datasets`** – Provides ready-to-use NLP/ML datasets from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "%pip install --upgrade \"pinecone>=5,<6\" langchain langchain-community langchain-pinecone langchain_groq datasets==3.5.0 sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00464f",
   "metadata": {},
   "source": [
    "## Load API Keys from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22aa0b2",
   "metadata": {},
   "source": [
    "### What is env file?\n",
    "\n",
    "* A .env file is a simple text file that stores environment variables (like API keys and secrets) in key=value format.\n",
    "* Example content of a .env file:\n",
    "* PINECONE_API_KEY=your_pinecone_api_key_here\n",
    "* GROQ_API_KEY=your_groq_api_key_here\n",
    "\n",
    "* It helps keep sensitive information out of your code and makes it easier to manage secrets securely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a4c86",
   "metadata": {},
   "source": [
    "**Imports tools** to:\n",
    "\n",
    "* Use environment variables (`os`)\n",
    "* Load values from a `.env` file (`load_dotenv`)\n",
    "* **os** is a Python built-in module that lets your code interact with the operating system (like Windows, macOS, Linux).\n",
    "---\n",
    "* **Loads the `.env` file** so Python can use the secret keys stored in it (like API keys).\n",
    "* **Gets the values** of `PINECONE_API_KEY` and `GROQ_API_KEY` from the `.env` file.\n",
    "* **In Short:** This code **reads your secret keys from a `.env` file** so you don’t have to write them directly in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08e6bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone API Key Loaded Successfully\n",
      "✅ Groq API Key Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the keys\n",
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded properly\n",
    "if pinecone_key:\n",
    "    print(\"✅ Pinecone API Key Loaded Successfully\")\n",
    "else:\n",
    "    print(\"❌ Pinecone API Key NOT Loaded\")\n",
    "\n",
    "if groq_key:\n",
    "    print(\"✅ Groq API Key Loaded Successfully\")\n",
    "else:\n",
    "    print(\"❌ Groq API Key NOT Loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1c075",
   "metadata": {},
   "source": [
    "## What is `langchain_groq`?\n",
    "\n",
    "`langchain_groq` is a **LangChain integration** that lets you **connect to Groq’s LLMs** (like LLaMA3) easily.\n",
    "\n",
    "Think of it as a **bridge between LangChain and Groq’s fast language models**.\n",
    "\n",
    "---\n",
    "\n",
    "## What is `ChatGroq`?\n",
    "\n",
    "`ChatGroq` is a **class (tool)** inside `langchain_groq`.\n",
    "\n",
    "It lets you:\n",
    "\n",
    "* **Send prompts** to Groq-hosted models\n",
    "* **Receive responses** from those models\n",
    "* Use these models in your **LangChain app**, like chatbots, RAG, agents, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we use this?**\n",
    "\n",
    "Instead of manually setting up HTTP requests to Groq’s API, `ChatGroq` makes it **super easy**:\n",
    "\n",
    "* Lets you talk to a specific Groq model (`llama3-8b-8192`)\n",
    "* Works smoothly with LangChain tools (retrievers, chains, memory, etc.)\n",
    "* Connects securely with your `groq_api_key`\n",
    "\n",
    "---\n",
    "\n",
    "#### In Simple Words\n",
    "\n",
    "* `langchain_groq` lets LangChain talk to Groq.\n",
    "* `ChatGroq` is the tool that helps you **chat with Groq’s AI model** using your API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "990bfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# \"llama3-70b-8192\" has been deprecated, using \"llama-3.3-70b-versatile\"\n",
    "\n",
    "chat = ChatGroq(\n",
    "    groq_api_key=groq_key,\n",
    "    model_name=\"llama-3.3-70b-versatile\"  # Correct model name used by Groq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09520cec",
   "metadata": {},
   "source": [
    "Groq uses the **same chat structure as OpenAI** because it runs **OpenAI-compatible models** like `llama3`, `mixtral`, etc.\n",
    "So just like OpenAI, chats with Groq **typically look like this in plain text**:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "User: Hi, how are you?\n",
    "Assistant: I'm doing well! How can I assist you today?\n",
    "User: What is quantum computing?\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "---\n",
    "\n",
    "**In Code (OpenAI/Groq-compatible format):**\n",
    "\n",
    "When using the API (like with `ChatGroq` or `ChatOpenAI` in LangChain), you use this structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**In LangChain (message objects):**\n",
    "\n",
    "LangChain wraps those into **message classes**, like:\n",
    "\n",
    "```\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What is quantum computing?\")\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21b712",
   "metadata": {},
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "Then you pass them to the model:\n",
    "\n",
    "```\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8254e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert physics tutor.\"),\n",
    "    HumanMessage(content=\"Hello AI, can you help me learn about quantum mechanics?\"),\n",
    "    AIMessage(content=\"Absolutely! Quantum mechanics is a fundamental theory in physics describing nature at the smallest scales. What would you like to know?\"),\n",
    "    HumanMessage(content=\"What is the uncertainty principle?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aab48e",
   "metadata": {},
   "source": [
    "We generate the next response from the AI by passing these messages to the `ChatGroq` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ddade",
   "metadata": {},
   "source": [
    "Like saying to the AI:\n",
    "\n",
    "“Here’s what has been said so far — now tell me what the AI should say next.”\n",
    "\n",
    "LangChain then handles formatting and sending this to the LLM backend, and res stores the AI’s next reply.\n",
    "\n",
    "**In Short:**\n",
    "* You define a conversation (via messages).\n",
    "* Call the LLM using chat(messages).\n",
    "* Get a response back — stored in res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "109d2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = chat(messages)\n",
    "\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d420165f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The uncertainty principle is a fundamental concept in quantum mechanics, introduced by Werner Heisenberg in 1927. It states that it\\'s impossible to know certain properties of a subatomic particle, such as its position (x) and momentum (p), simultaneously with infinite precision.\\n\\nMathematically, this is expressed as:\\n\\nΔx \\\\* Δp >= h/4π\\n\\nwhere Δx is the uncertainty in position, Δp is the uncertainty in momentum, and h is the Planck constant.\\n\\nIn simpler terms, the more precisely you try to measure a particle\\'s position, the less precisely you can know its momentum, and vice versa. This is not due to limitations in measurement technology, but rather a fundamental property of the quantum world.\\n\\nFor example, if you try to measure the position of an electron very precisely, you would need to use a high-energy photon to \"illuminate\" it. However, this photon would disturb the electron\\'s momentum, making it impossible to know its momentum precisely at the same time.\\n\\nThe uncertainty principle has far-reaching implications in quantum mechanics, including:\\n\\n1. **Wave-particle duality**: particles can exhibit both wave-like and particle-like behavior, but not simultaneously.\\n2. **Quantization**: certain properties, like energy, can only take on specific discrete values.\\n3. **Limits on measurement**: there are fundamental limits to how precisely we can measure certain properties.\\n\\nThe uncertainty principle is a mind-bending concept that challenges our classical intuition about the behavior of particles. Do you have any specific questions about it, or would you like me to elaborate further?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 94, 'total_tokens': 416, 'completion_time': 0.748876998, 'prompt_time': 0.004848817, 'queue_time': 0.048279803, 'total_time': 0.753725815}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_34d416ee39', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d83ff6bb-5310-4f22-b395-cb668990aa73-0', usage_metadata={'input_tokens': 94, 'output_tokens': 322, 'total_tokens': 416})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f108837",
   "metadata": {},
   "source": [
    "To see the models reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "748f8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The uncertainty principle is a fundamental concept in quantum mechanics, introduced by Werner Heisenberg in 1927. It states that it's impossible to know certain properties of a subatomic particle, such as its position (x) and momentum (p), simultaneously with infinite precision.\n",
      "\n",
      "Mathematically, this is expressed as:\n",
      "\n",
      "Δx \\* Δp >= h/4π\n",
      "\n",
      "where Δx is the uncertainty in position, Δp is the uncertainty in momentum, and h is the Planck constant.\n",
      "\n",
      "In simpler terms, the more precisely you try to measure a particle's position, the less precisely you can know its momentum, and vice versa. This is not due to limitations in measurement technology, but rather a fundamental property of the quantum world.\n",
      "\n",
      "For example, if you try to measure the position of an electron very precisely, you would need to use a high-energy photon to \"illuminate\" it. However, this photon would disturb the electron's momentum, making it impossible to know its momentum precisely at the same time.\n",
      "\n",
      "The uncertainty principle has far-reaching implications in quantum mechanics, including:\n",
      "\n",
      "1. **Wave-particle duality**: particles can exhibit both wave-like and particle-like behavior, but not simultaneously.\n",
      "2. **Quantization**: certain properties, like energy, can only take on specific discrete values.\n",
      "3. **Limits on measurement**: there are fundamental limits to how precisely we can measure certain properties.\n",
      "\n",
      "The uncertainty principle is a mind-bending concept that challenges our classical intuition about the behavior of particles. Do you have any specific questions about it, or would you like me to elaborate further?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d2d69",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36c75ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum entanglement is a fascinating phenomenon in quantum mechanics. It's a fundamental aspect of the quantum world, and it has been extensively experimentally confirmed.\n",
      "\n",
      "**What is quantum entanglement?**\n",
      "\n",
      "Quantum entanglement occurs when two or more particles become correlated in such a way that their properties, such as spin, momentum, or energy, are connected, even when they are separated by large distances. This means that measuring the state of one particle instantly affects the state of the other entangled particles, regardless of the distance between them.\n",
      "\n",
      "**Key features of entanglement:**\n",
      "\n",
      "1. **Correlation**: Entangled particles are correlated, meaning that the state of one particle is dependent on the state of the other.\n",
      "2. **Non-locality**: Entangled particles can be separated by arbitrary distances, and the correlation between them remains.\n",
      "3. **Instantaneous interaction**: Measuring the state of one particle instantly affects the state of the other entangled particles, regardless of the distance between them.\n",
      "\n",
      "**Example: Entangled photons**\n",
      "\n",
      "Imagine two photons, A and B, entangled in such a way that their polarization (a property of light) is correlated. If photon A is polarized vertically, photon B will be polarized horizontally, and vice versa. Now, suppose you separate the photons by a large distance, say, millions of kilometers. If you measure the polarization of photon A, you will instantly determine the polarization of photon B, regardless of the distance between them.\n",
      "\n",
      "**Entanglement and the EPR paradox**\n",
      "\n",
      "In 1935, Einstein, Podolsky, and Rosen (EPR) proposed a thought experiment to demonstrate the apparent absurdity of quantum mechanics. They suggested that if two particles are entangled, measuring the state of one particle would instantly affect the state of the other, regardless of distance. This seemed to imply faster-than-light communication, which contradicts the fundamental principles of special relativity.\n",
      "\n",
      "However, subsequent experiments have consistently confirmed the predictions of quantum mechanics, demonstrating the reality of entanglement and its non-local nature.\n",
      "\n",
      "**Quantum entanglement and its applications:**\n",
      "\n",
      "1. **Quantum computing**: Entanglement is a key resource for quantum computing, as it enables the creation of quantum gates and quantum algorithms.\n",
      "2. **Quantum cryptography**: Entanglement-based quantum cryptography provides secure communication over long distances.\n",
      "3. **Quantum teleportation**: Entanglement enables the transfer of quantum information from one particle to another, without physical transport of the particles themselves.\n",
      "\n",
      "**Mind-bending implications:**\n",
      "\n",
      "1. **Non-locality**: Entanglement challenges our classical understanding of space and time.\n",
      "2. **Reality of the wave function**: Entanglement suggests that the wave function, a mathematical description of a quantum system, is a real, physical entity.\n",
      "3. **Limits of local realism**: Entanglement demonstrates the failure of local realism, a concept that assumes physical properties are predetermined and independent of measurement.\n",
      "\n",
      "Quantum entanglement is a fascinating and counterintuitive phenomenon that has been extensively experimentally confirmed. Its implications continue to inspire research and debate in the fields of quantum mechanics, philosophy, and beyond.\n",
      "\n",
      "Do you have any specific questions about entanglement or would you like me to elaborate on any of these points?\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you explain quantum entanglement?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c7774",
   "metadata": {},
   "source": [
    "## Dealing with Hallucinations\n",
    "\n",
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about Deepseek R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0b2cfb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no GPT-5. The current models in the GPT series are:\n",
      "\n",
      "1. GPT-1 (2018): Developed by OpenAI, a research organization founded by Elon Musk, Sam Altman, and others.\n",
      "2. GPT-2 (2019): Also developed by OpenAI, with improvements over the original GPT model.\n",
      "3. GPT-3 (2020): Developed by OpenAI, with significant advancements in natural language processing and generation capabilities.\n",
      "4. GPT-4 (2023): The latest model in the series, also developed by OpenAI, with further improvements in performance and capabilities.\n",
      "\n",
      "Note that I'm an AI designed to assist and communicate with users, but I'm not affiliated with OpenAI or any specific organization. I exist to provide information and answer questions to the best of my abilities, based on my training and knowledge.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Who founded GPT-5?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d2b6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794be3e",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10521",
   "metadata": {},
   "source": [
    "## Alternate Way : Source Knowledge\n",
    "\n",
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the Deepseek question. We can take the paper abstract from the [Deepseek R1 paper](https://arxiv.org/abs/2501.12948)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9af49e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \" GPT-5 was founded by a team of researchers and engineers at OpenAI, building upon the advancements of previous models like GPT-4. The development focused on enhancing natural language understanding and generation capabilities, aiming to create a more versatile and powerful AI system.\" \\\n",
    "    \" The team included experts in machine learning, neuroscience, and computational linguistics, working collaboratively to push the boundaries of AI technology. \" \\\n",
    "    \"Their goal was to create a model that could not only understand and generate human-like text but also assist in a wide range of applications, from creative writing to complex problem-solving.\" \\\n",
    "\n",
    "    \" GPT-5's development was marked by significant advancements in model architecture, training techniques, and data utilization, making it one of the most sophisticated AI models available today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffaa08",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40297f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about GPT-5?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf6bb0",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e947bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f993466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5 is special because it represents a significant advancement in natural language understanding and generation capabilities. Its development focused on creating a more versatile and powerful AI system that can not only understand and generate human-like text but also assist in a wide range of applications, from creative writing to complex problem-solving. The model's architecture, training techniques, and data utilization have been greatly improved, making it one of the most sophisticated AI models available today. This allows GPT-5 to be highly effective in various tasks, setting it apart from its predecessors and making it a cutting-edge technology in the field of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b235b09",
   "metadata": {},
   "source": [
    "## How do we get this information in the first place?\n",
    "\n",
    "The quality of this answer is made possible thanks to augmenting our query with external knowledge (source knowledge) retrieved from our Llama 2 arXiv corpus. We'll now build a vector index from \"jamescalam/llama-2-arxiv-papers-chunked\" so the chatbot can ground its answers in those papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b79b1",
   "metadata": {},
   "source": [
    "This is where Pinecone and vector databases come into play, as they can help us here too. But first, we'll need a dataset from our AI ArXiv corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b633b59",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "We'll import our knowledge base from the Hugging Face Datasets library. For this assignment we will use the \"jamescalam/llama-2-arxiv-papers-chunked\" dataset. This dataset contains chunked (~300 tokens) extracts from the Llama 2 research paper and closely related papers (identified via references). It has ~4.84k rows and includes fields like `doi`, `chunk-id`, `chunk`, `title`, `summary`, `authors`, `categories`, `source`, and more — ideal for Llama 2-focused RAG experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9ce7d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c9178a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81d343",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from Llama 2 and related arXiv papers. Each entry is a chunk (~300 tokens) with helpful fields such as `doi`, `chunk-id`, `chunk` (text), `title`, `summary`, `authors`, `categories`, and `source` (PDF URL). This makes it well-suited to build a RAG knowledge base for answering questions about Llama 2 model architecture, training, safety, and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a0ede",
   "metadata": {},
   "source": [
    "## Building the Knowledge Base\n",
    "\n",
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our Pinecone client, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5c308ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize client\n",
    "pc = Pinecone(api_key=pinecone_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78b484",
   "metadata": {},
   "source": [
    "Delete the old one to save the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4aa6f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing index to delete\n"
     ]
    }
   ],
   "source": [
    "index_name = \"rag-chunked\" \n",
    "\n",
    "try:\n",
    "    pc.delete_index(index_name)\n",
    "    print(f\"Deleted existing index: {index_name}\")\n",
    "except:\n",
    "    print(f\"No existing index to delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "545e1533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"rag-chunked\",\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"rag-chunked-izpvf0z.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,\n",
    "    metric=\"dotproduct\",             \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df29057",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will HuggingFace's `sentence-transformers/all-MiniLM-L6-v2` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd405ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9229b3d",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5daf97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Quantum entanglement is a physical phenomenon that occurs when pairs or groups of particles are generated, interact, or share spatial proximity in ways such that the quantum state of each particle cannot be described independently of the state of the others, even when the particles are separated by a large distance.\",\n",
    "    \"The uncertainty principle, formulated by Werner Heisenberg, states that certain pairs of physical properties, like position and momentum, cannot both be known to arbitrary precision. The more precisely one property is known, the less precisely the other can be known.\",\n",
    "    \"String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. It aims to reconcile quantum mechanics and general relativity, potentially providing a unified description of all fundamental forces and forms of matter.\"\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ea6f5",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 384-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all of our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88b28f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [06:55<00:00,  8.48s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Get the Pinecone index object\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'], 'source': x['source']} for _, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone \n",
    "    vectors = [\n",
    "        {\n",
    "            'id': _id,\n",
    "            'values': vec,\n",
    "            'metadata': meta\n",
    "        }\n",
    "        for _id, vec, meta in zip(ids, embeds, metadata)\n",
    "    ]\n",
    "    index.upsert(vectors=vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e4fa5",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f56bb113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e988a",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86c59e",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to link that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb42eae",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3b9846ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embed_model,\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799256a3",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3214e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2307.09288-199', metadata={'source': 'http://arxiv.org/pdf/2307.09288'}, page_content='Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.'),\n",
       " Document(id='2307.09288-14', metadata={'source': 'http://arxiv.org/pdf/2307.09288'}, page_content='our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication'),\n",
       " Document(id='2307.09288-319', metadata={'source': 'http://arxiv.org/pdf/2307.09288'}, page_content='Evaluation Results\\nSee evaluations for pretraining (Section 2); ﬁne-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations (Section 5.2)\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\\napplications of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle, developers should perform safety testing and tuning tailored to their\\nspeciﬁc applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card for L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle .\\n77')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the key design choices and safety considerations in Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcb169",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to link the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25a15337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query concisely and cite which paper(s) you used when possible.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a526be0",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "769766aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query concisely and cite which paper(s) you used when possible.\n",
      "\n",
      "    Contexts:\n",
      "    Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\n",
      "Chauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\n",
      "Yazdan, Elisa Garcia Anzano, and Natascha Parks.\n",
      "•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\n",
      "46\n",
      "•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\n",
      "Llama team who helped get this work started.\n",
      "•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\n",
      "paper.\n",
      "•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\n",
      "internal demo.\n",
      "•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\n",
      "Laurens van der Maaten, Jason Weston, and Omer Levy.\n",
      "our responsible release strategy can be found in Section 5.3.\n",
      "Theremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\n",
      "(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\n",
      "work (Section 6), and conclusions (Section 7).\n",
      "‡https://ai.meta.com/resources/models-and-libraries/llama/\n",
      "§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\n",
      "¶https://ai.meta.com/llama\n",
      "‖https://github.com/facebookresearch/llama\n",
      "4\n",
      "Figure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\n",
      "availableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication\n",
      "Evaluation Results\n",
      "See evaluations for pretraining (Section 2); ﬁne-tuning (Section 3); and safety (Section 4).\n",
      "Ethical Considerations and Limitations (Section 5.2)\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is a new technology that carries risks with use. Testing conducted to date has been in\n",
      "English, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle’s potential outputs cannot be predicted in advance, and the model may in some instances\n",
      "produceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\n",
      "applications of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle, developers should perform safety testing and tuning tailored to their\n",
      "speciﬁc applications of the model. Please see the Responsible Use Guide available available at\n",
      "https://ai.meta.com/llama/responsible-user-guide\n",
      "Table 52: Model card for L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle .\n",
      "77\n",
      "\n",
      "    Query: What are the key design choices and safety considerations in Llama 2?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59e5e8",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d8a399de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core contributions of Llama 2 include the development and release of a family of pretrained and fine-tuned large language models (LLMs) at scales up to 70B parameters. The benchmarking results show that the Llama 2 models, specifically L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, generally perform better than existing open-source models and are on par with some closed-source models, such as ChatGPT, BARD, and Claude, in terms of helpfulness and safety (as evaluated in Section 5 of the paper).\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt using the new dataset\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\"Summarize the core contributions and benchmarking results of Llama 2.\")\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d917fe",
   "metadata": {},
   "source": [
    "We can continue with another Deepseek R1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cadefe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Llama 2 paper recommends several safety mitigations and usage restrictions, including:\n",
      "\n",
      "1. **Safety testing and tuning**: Developers should perform safety testing and tuning tailored to their specific applications of the model before deploying it (Section 5.2).\n",
      "2. **Responsible Use Guide**: Users are encouraged to follow the Responsible Use Guide available at https://ai.meta.com/llama/responsible-user-guide.\n",
      "3. **License and Acceptable Use Policy compliance**: Users must comply with the terms of the provided license and the Acceptable Use Policy, which prohibit uses that would violate applicable policies, laws, rules, and regulations (Section 5.3).\n",
      "4. **Cautious use of pretrained models**: Users of the pretrained models should be particularly cautious and take extra steps in tuning and deployment (Section 5.2).\n",
      "5. **Red teaming**: The release of the 34B model is delayed due to a lack of time to sufficiently red team, highlighting the importance of thorough testing and evaluation (Section 5.3).\n",
      "\n",
      "These recommendations aim to mitigate potential risks associated with the use of Llama 2, such as inaccurate or objectionable responses, and ensure responsible use of the model. (Used Sections 5.2 and 5.3 of the paper)\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"What safety mitigations and usage restrictions are recommended in the Llama 2 paper?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef84ba",
   "metadata": {},
   "source": [
    "You can continue asking questions about Deepseek R1, but once you're done you can delete the index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49ac1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buildables-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
